{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In Colab, make sure you run cells in this order:\n",
        "\n",
        "1. Imports\n",
        "2. Config class\n",
        "3. StockDataset class\n",
        "4. sLSTMCell class (with state initialization)\n",
        "5. mLSTMCell class (with state initialization)\n",
        "6. xLSTMBlock class (with sequential processing)\n",
        "7. xLSTM class\n",
        "8. Trainer class\n",
        "9. main() function\n",
        "10. Call main()\n"
      ],
      "metadata": {
        "id": "HJgHcLEIkKME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas numpy tqdm wandb matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwPzUT1sefG9",
        "outputId": "c8c3712b-3ddb-483a-ae03-92888251f60b"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vVHha3mpeIA9"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Single config class for everything\"\"\"\n",
        "    # Model\n",
        "    num_features = 5        # Close, Volume, Open, High, Low\n",
        "    embed_dim = 64\n",
        "    num_blocks = 2\n",
        "    block_types = ['slstm']  # alternating\n",
        "\n",
        "    # Training\n",
        "    batch_size = 2\n",
        "    learning_rate = 3e-4\n",
        "    num_epochs = 10\n",
        "    seq_length = 50       # Lookback window\n",
        "    prediction_horizon = 1  # Predict N days ahead\n",
        "    normalize_data = True\n",
        "    grad_clip = 1.0\n",
        "\n",
        "    # Checkpoint Resume\n",
        "    resume_from_checkpoint = 9  # Set to None to start from scratch\n",
        "\n",
        "    # Paths\n",
        "    train_path = \"/content/Panantir-5Y.csv\"\n",
        "    val_path = \"/content/Panantir-5Y.csv\"\n",
        "    checkpoint_dir = \"checkpoints\"\n",
        "\n",
        "    # WandB\n",
        "    use_wandb = False\n",
        "    wandb_project = \"xlstm-stock-prediction\"\n",
        "    wandb_run_name = None  # Auto-generated if None\n",
        "\n",
        "    # Device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "y7Dx8i-ieLHa"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StockDataset(Dataset):\n",
        "    \"\"\"Load CSV stock data and create sequences for time-series prediction\"\"\"\n",
        "\n",
        "    def __init__(self, path, seq_length=50, prediction_horizon=1, normalize=True):\n",
        "        self.seq_length = seq_length\n",
        "        self.prediction_horizon = prediction_horizon\n",
        "\n",
        "        # Load and clean data\n",
        "        df = pd.read_csv(path)\n",
        "        df = df.sort_values('Date')  # Ensure chronological order\n",
        "\n",
        "        # Clean price columns (remove $ and commas)\n",
        "        price_cols = ['Close/Last', 'Open', 'High', 'Low']\n",
        "        for col in price_cols:\n",
        "            df[col] = df[col].str.replace('$', '').str.replace(',', '').astype(float)\n",
        "\n",
        "        # Select features: Close, Volume, Open, High, Low\n",
        "        features = df[['Close/Last', 'Volume', 'Open', 'High', 'Low']].values\n",
        "\n",
        "        # Normalize\n",
        "        if normalize:\n",
        "            self.mean = features.mean(axis=0)\n",
        "            self.std = features.std(axis=0)\n",
        "            features = (features - self.mean) / (self.std + 1e-8)\n",
        "\n",
        "        # Create sequences\n",
        "        self.sequences = []\n",
        "        for i in range(len(features) - seq_length - prediction_horizon + 1):\n",
        "            input_seq = features[i:i + seq_length]\n",
        "            target_seq = features[i + seq_length:i + seq_length + prediction_horizon]\n",
        "            self.sequences.append((input_seq, target_seq))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target_seq = self.sequences[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_seq, dtype=torch.float32),\n",
        "            'labels': torch.tensor(target_seq, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)"
      ],
      "metadata": {
        "id": "sS_VfdV3eOU0"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sLSTMCell(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim  # ADD THIS\n",
        "        self.W_i = nn.Linear(dim*2, dim)\n",
        "        self.W_f = nn.Linear(dim*2, dim)\n",
        "        self.W_o = nn.Linear(dim*2, dim)\n",
        "        self.W_z = nn.Linear(dim*2, dim)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        # Initialize state if None\n",
        "        if state is None:\n",
        "            batch_size = x.size(0)\n",
        "            device = x.device\n",
        "            h = torch.zeros(batch_size, self.dim, device=device)\n",
        "            c = torch.zeros(batch_size, self.dim, device=device)\n",
        "            n = torch.zeros(batch_size, self.dim, device=device)\n",
        "            m = torch.zeros(batch_size, self.dim, device=device)\n",
        "            state = (h, c, n, m)\n",
        "\n",
        "        h, c, n, m = state\n",
        "        combined = torch.cat([x, h], -1)\n",
        "\n",
        "        i_t = self.W_i(combined)\n",
        "        f_t = self.W_f(combined)\n",
        "        o_t = self.W_o(combined)\n",
        "        z = torch.tanh(self.W_z(combined))\n",
        "\n",
        "        m_new = torch.max(f_t + m, i_t)\n",
        "        i = torch.exp(i_t - m_new)\n",
        "        f = torch.exp(f_t + m - m_new)\n",
        "\n",
        "        c_new = f * c + i * z\n",
        "        n_new = f * n + i\n",
        "        h_new = torch.sigmoid(o_t) * (c_new / (n_new + 1e-6))\n",
        "\n",
        "        return h_new, (h_new, c_new, n_new, m_new)"
      ],
      "metadata": {
        "id": "G0mLdVLwePrk"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTMCell(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(dim, dim)\n",
        "        self.W_k = nn.Linear(dim, dim)\n",
        "        self.W_v = nn.Linear(dim, dim)\n",
        "        self.W_i = nn.Linear(dim, dim)\n",
        "        self.W_f = nn.Linear(dim, dim)\n",
        "        self.W_o = nn.Linear(dim, dim)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        # Initialize state if None\n",
        "        if state is None:\n",
        "            batch_size = x.size(0)\n",
        "            device = x.device\n",
        "            C = torch.zeros(batch_size, self.dim, self.dim, device=device)\n",
        "            n = torch.zeros(batch_size, self.dim, device=device)\n",
        "            m = torch.zeros(batch_size, self.dim, device=device)\n",
        "            state = (C, n, m)\n",
        "\n",
        "        C, n, m = state\n",
        "\n",
        "        q = self.W_q(x)\n",
        "        k = self.W_k(x) / (self.dim ** 0.5)\n",
        "        v = self.W_v(x)\n",
        "        i_t = self.W_i(x)\n",
        "        f_t = self.W_f(x)\n",
        "\n",
        "        m_new = torch.max(f_t + m, i_t)\n",
        "        i = torch.exp(i_t - m_new).unsqueeze(-1)\n",
        "        f = torch.exp(f_t + m - m_new).unsqueeze(-1)\n",
        "\n",
        "        C_new = f * C + i * torch.bmm(v.unsqueeze(-1), k.unsqueeze(-2))\n",
        "        n_new = f.squeeze(-1) * n + i.squeeze(-1) * k\n",
        "\n",
        "        h = torch.sigmoid(self.W_o(x)) * torch.bmm(C_new, q.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        return h, (C_new, n_new, m_new)"
      ],
      "metadata": {
        "id": "aSk4fFNKlmVs"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class xLSTMBlock(nn.Module):\n",
        "    def __init__(self, dim, cell_type):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.cell = sLSTMCell(dim) if cell_type == 'slstm' else mLSTMCell(dim)\n",
        "        self.cell_type = cell_type  # ADD THIS\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim*4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Reset state to None (each block maintains its own state)\n",
        "        state = None\n",
        "\n",
        "        # Process sequence step by step\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            x_t = x[:, t, :]  # [batch, dim]\n",
        "            normed = self.norm1(x_t)\n",
        "            h, state = self.cell(normed, state)\n",
        "\n",
        "            # Add residual and FFN\n",
        "            out = x_t + h\n",
        "            out = out + self.ffn(self.norm2(out))\n",
        "            outputs.append(out)\n",
        "\n",
        "        # Stack outputs back to [batch, seq_len, dim]\n",
        "        x = torch.stack(outputs, dim=1)\n",
        "\n",
        "        return x, state"
      ],
      "metadata": {
        "id": "xHXYMUqIlgJM"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class xLSTM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(config.num_features, config.embed_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            xLSTMBlock(config.embed_dim, config.block_types[i % len(config.block_types)])\n",
        "            for i in range(config.num_blocks)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config.embed_dim)\n",
        "        self.head = nn.Linear(config.embed_dim, config.num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        state = None\n",
        "        for block in self.blocks:\n",
        "            x, state = block(x, state)\n",
        "        x = self.head(self.norm(x))\n",
        "        return x[:, -1:, :]  # Return only last timestep prediction"
      ],
      "metadata": {
        "id": "5rbi-pSdj_Aq"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-14aquHumgFb",
        "outputId": "f7f5942c-8468-458d-fd73-45588ee62fa8"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "924"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, config):\n",
        "        self.model = model.to(config.device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_mae': [],\n",
        "            'val_rmse': []\n",
        "        }\n",
        "\n",
        "        Path(config.checkpoint_dir).mkdir(exist_ok=True)\n",
        "\n",
        "        # Initialize WandB\n",
        "        if config.use_wandb:\n",
        "            wandb.init(\n",
        "                project=config.wandb_project,\n",
        "                name=config.wandb_run_name,\n",
        "                config={\n",
        "                    \"num_features\": config.num_features,\n",
        "                    \"embed_dim\": config.embed_dim,\n",
        "                    \"num_blocks\": config.num_blocks,\n",
        "                    \"block_types\": config.block_types,\n",
        "                    \"batch_size\": config.batch_size,\n",
        "                    \"learning_rate\": config.learning_rate,\n",
        "                    \"num_epochs\": config.num_epochs,\n",
        "                    \"seq_length\": config.seq_length,\n",
        "                    \"prediction_horizon\": config.prediction_horizon,\n",
        "                }\n",
        "            )\n",
        "            wandb.watch(self.model, log=\"all\", log_freq=100)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "            \"\"\"Load model and optimizer state from checkpoint\"\"\"\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=self.config.device)\n",
        "\n",
        "            self.model.load_state_dict(checkpoint['model_state'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "\n",
        "           # Load history if available\n",
        "            if 'history' in checkpoint:\n",
        "                self.history = checkpoint['history']\n",
        "\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "            print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "            return start_epoch\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot loss\n",
        "        ax1.plot(epochs, self.history['train_loss'], 'b-o', label='Train Loss')\n",
        "        ax1.plot(epochs, self.history['val_loss'], 'r-o', label='Val Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss (MSE)')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Plot metrics\n",
        "        ax2.plot(epochs, self.history['val_mae'], 'g-s', label='Val MAE')\n",
        "        ax2.plot(epochs, self.history['val_rmse'], 'm-^', label='Val RMSE')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Error')\n",
        "        ax2.set_title('Validation Metrics')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_history.png', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(tqdm(self.train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
        "            inputs = batch['input_ids'].to(self.config.device)\n",
        "            labels = batch['labels'].to(self.config.device)\n",
        "\n",
        "            predictions = self.model(inputs)\n",
        "            loss = self.criterion(predictions, labels)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Log to WandB every 10 steps\n",
        "            if self.config.use_wandb and step % 10 == 0:\n",
        "                wandb.log({\n",
        "                    \"train_loss_step\": loss.item(),\n",
        "                    \"epoch\": epoch,\n",
        "                    \"step\": epoch * len(self.train_loader) + step\n",
        "                })\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_mae = 0\n",
        "\n",
        "        for batch in self.val_loader:\n",
        "            inputs = batch['input_ids'].to(self.config.device)\n",
        "            labels = batch['labels'].to(self.config.device)\n",
        "\n",
        "            predictions = self.model(inputs)\n",
        "            loss = self.criterion(predictions, labels)\n",
        "            mae = torch.abs(predictions - labels).mean()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_mae += mae.item()\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        avg_mae = total_mae / len(self.val_loader)\n",
        "        rmse = np.sqrt(avg_loss)\n",
        "\n",
        "        return avg_loss, avg_mae, rmse\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def plot_predictions(self, num_samples=100):\n",
        "        \"\"\"Plot actual vs predicted prices\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_actuals = []\n",
        "\n",
        "        # Get predictions from validation set\n",
        "        for i, batch in enumerate(self.val_loader):\n",
        "            if len(all_predictions) >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs = batch['input_ids'].to(self.config.device)\n",
        "            labels = batch['labels'].to(self.config.device)\n",
        "\n",
        "            predictions = self.model(inputs)\n",
        "\n",
        "            all_predictions.append(predictions.cpu())\n",
        "            all_actuals.append(labels.cpu())\n",
        "\n",
        "        # Concatenate all batches\n",
        "        predictions = torch.cat(all_predictions, dim=0)[:num_samples]\n",
        "        actuals = torch.cat(all_actuals, dim=0)[:num_samples]\n",
        "\n",
        "        # Extract closing price (feature index 0)\n",
        "        pred_close = predictions[:, 0, 0].numpy()\n",
        "        actual_close = actuals[:, 0, 0].numpy()\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        # Top plot: Predictions vs Actuals\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(actual_close, label='Actual Price', color='blue', linewidth=2, alpha=0.7)\n",
        "        plt.plot(pred_close, label='Predicted Price', color='red', linewidth=2, alpha=0.7)\n",
        "        plt.title('Stock Price Predictions vs Actuals', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Time Steps')\n",
        "        plt.ylabel('Normalized Price')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Bottom plot: Prediction Error\n",
        "        plt.subplot(2, 1, 2)\n",
        "        errors = pred_close - actual_close\n",
        "        plt.plot(errors, color='purple', linewidth=1.5, alpha=0.7)\n",
        "        plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "        plt.fill_between(range(len(errors)), errors, 0, alpha=0.3, color='purple')\n",
        "        plt.title('Prediction Error Over Time', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Time Steps')\n",
        "        plt.ylabel('Error')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add statistics\n",
        "        mae = np.abs(errors).mean()\n",
        "        rmse = np.sqrt((errors**2).mean())\n",
        "        textstr = f'MAE: {mae:.4f}\\nRMSE: {rmse:.4f}'\n",
        "        plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes,\n",
        "                 fontsize=10, verticalalignment='top',\n",
        "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('predictions_vs_actuals.png', dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        if self.config.use_wandb:\n",
        "            wandb.log({\"predictions\": wandb.Image('predictions_vs_actuals.png')})\n",
        "\n",
        "    def train(self):\n",
        "        start_epoch = 0\n",
        "\n",
        "        if self.config.resume_from_checkpoint is not None:\n",
        "            checkpoint_path = f\"{self.config.checkpoint_dir}/checkpoint_epoch_{self.config.resume_from_checkpoint}.pt\"\n",
        "            start_epoch = self.load_checkpoint(checkpoint_path)\n",
        "\n",
        "        for epoch in range(start_epoch, self.config.num_epochs):\n",
        "            train_loss = self.train_epoch(epoch)\n",
        "            val_loss, val_mae, val_rmse = self.validate()\n",
        "\n",
        "            # Store history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_mae'].append(val_mae)\n",
        "            self.history['val_rmse'].append(val_rmse)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.6f}, \"\n",
        "                  f\"Val Loss={val_loss:.6f}, Val MAE={val_mae:.6f}, Val RMSE={val_rmse:.6f}\")\n",
        "\n",
        "            # Log to WandB\n",
        "            if self.config.use_wandb:\n",
        "                wandb.log({\n",
        "                    \"train_loss_epoch\": train_loss,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"val_mae\": val_mae,\n",
        "                    \"val_rmse\": val_rmse,\n",
        "                    \"epoch\": epoch + 1\n",
        "                })\n",
        "\n",
        "            # Save checkpoint\n",
        "            checkpoint_path = f\"{self.config.checkpoint_dir}/checkpoint_epoch_{epoch}.pt\"\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state': self.model.state_dict(),\n",
        "                'optimizer_state': self.optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'history': self.history,  # ADDED: Save history\n",
        "            }, checkpoint_path)\n",
        "\n",
        "            # Log checkpoint to WandB\n",
        "            if self.config.use_wandb:\n",
        "                wandb.save(checkpoint_path)\n",
        "\n",
        "        # Plot training history at the end\n",
        "        self.plot_training_history()\n",
        "\n",
        "        # Plot predictions vs actuals\n",
        "        self.plot_predictions(num_samples=100)\n",
        "\n",
        "        if self.config.use_wandb:\n",
        "            wandb.log({\"training_history\": wandb.Image('training_history.png')})\n",
        "            wandb.finish()"
      ],
      "metadata": {
        "id": "zIENAhpJeTIn"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    config = Config()\n",
        "\n",
        "    # Load data\n",
        "    train_dataset = StockDataset(\n",
        "        config.train_path,\n",
        "        seq_length=config.seq_length,\n",
        "        prediction_horizon=config.prediction_horizon,\n",
        "        normalize=config.normalize_data\n",
        "    )\n",
        "    val_dataset = StockDataset(\n",
        "        config.val_path,\n",
        "        seq_length=config.seq_length,\n",
        "        prediction_horizon=config.prediction_horizon,\n",
        "        normalize=config.normalize_data\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = xLSTM(config)\n",
        "    trainer = Trainer(model, train_loader, val_loader, config)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue75VxHveVeV",
        "outputId": "870ba279-12ca-4e42-8b58-b08dfd6ec61e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 133,573\n",
            "Loaded checkpoint from epoch 9\n",
            "Resuming training from epoch 10\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
